tf.math.sigmoid(z)
tf.math.tanh(z)
tf.nn.relu(z)

activation function to introduce non linearity

(linear activation functions produce linear decisions) activation function just allows the appeoximation of "arbitrarily complex functions"


dot product of weights, add bias, add non linearity -> steps for deep learning

dense layer = tf.keras.layers.Dense(units=([number of outputs]))

hidden layers can be added using dense


model = tf.keras.Sequential([
tf.keras.layers.Dense(n), tf.keras.layers.Dense(2)]) -> runs layers to propagate sequentially

loss function to optimize weights and biases 

calculating loss:

loss = tf.reduce_mean(tf.nn.softmax_corss_entropy_with_logits(y, predicted)) -> classification problem

for regression, use mean squared error loss ->
loss = tf.reduce_mean(tf.square(tf.subtract(y, predicted)))

loss optimization -> gradient descent finds lowest point in loss optimization function(based on weights) to reduce loss as much as possible

backpropagation already implemented

learning rate is defined by the size of the step taken in gradient descent to converge on the lcal minimum (least loss in loss function pltted by weights). If it is too large, you can easily overshoot, else you might get stuck in a local minima. This however can be dynamically solved by tensorflow.

many algorithms for gradient descent algorithm

SGD -> tf.keras.optimizers.SGD
Adam -> tf.keras.optimizers.Adam
Adadelta -> tf.keras.optimizers.Adadelta
Adagrad -> tf.keras.optimizers.Adagrad
RMSProp -> tf.keras.optimizers.RMSProp

example code:


""""""

import tensorflow as tf

model = tf.keras.Sequential([...])

while True: # do forever

	prediction = model(z)
	with tf.GradientTape() as tape:
		loss = compute_loss(y, prediction) -> produces non 2d loss function upon which gradient descent needs to be performed
	# update the weights using the gradient
	grads = tape.gradient(loss, model.trainable_variables)
	optimizer.apply_gradients(zip(grads, model.trainable_variables)) # next iteration, confidence will be better


deep learning overfitting:

47:43 in lecture has a good illustration

we dont want it to memorize the detection of just the training data (if you do train forever, it will do that). we want it to be generalizable so that it can be applied to new training data.

Drop out fixes this. sets some activations which forces network to rely on all nodes as a whole and not just one (we dont want a memorization channel. find multiple channels in model and makes it more robust (essential for regularization). typical dropout is 50%. 

tf.keras.layers.Dropout(p=0.5)

Also early stopping helps with regularization

look at 51.04 in lecture. YOu can graph both testing and training loss versus training iteration to see point at which the training data loss and testing data loss diverge(if it trains too far, it starts memorizing data). We dont want it to do much better on training data then testing data and so we can find the training iteration at which we want to stop training by graphing the two.

Link to lecture: https://www.youtube.com/watch?v=njKP3FqW3Sk&list=PLtBw6njQRU-rwp5__7C0oIVt26ZgjG9NI&index=1

Lecture 2 on RNNs

Deep Sequence Modeling - sequential processing of data (need previous data for future predictions)

input and output sizes are not always defined (words in a sentence can vary and maybe want to feed in sentences of different lengths to model)
	we can use fixed model to force it look at first n elements or
	use sequence as sets of counts (count number of times word appears and pass that into data) (doesnt save order though)
	

RNNs have loops where network takes where last output is being fed in as input 


CNN Notes: 

enter matrix of image (3 dimensional for rgb) (less dimensions in image makes computationally faster)

classification (binary output) vs regression gives value in range

end number of nodes needs to be number of possible situations. Result on each ending node is probability. 

dense layers every node connected to every other node

Image detection needs to have spacial structure such that it knows what is around each node so that it is not just predicting on counts of certain rgb values in an image.

Pass groups of pixels to each node fixes this as each node is also getting surrounding data. The patch operation is called convolution. 
ex. 4 byb 4 patch convolution means moving 2d window that looks at 4 pixels by 4 pixels, runs some set of weights on patch of pixels to extract local features. 

How does convolution extract features. convolution makes it look for similar "patches". see 14:00 in lecture of convolution. Different filters can help detect specific features (filter is just matrix of weights). see 16:45 in lecture to see how filter with image creates feature map. Different weights in filter can detect different things (sharpen, edge detection (canny (is that the same)). Can use multiple filters to extract different features. 

CNNs are neural network architectures:
input image -> convolution layer (filter feature maps)  -> maxpooling -> Dense Layers. 
Pooling: downsampling operation on each feature map

each neuron is only seeing a little patch when using a conv layers. 

mutiple filters on a single convolutional layer (convolutional layers creates multiple images based on each filter being run. convolutional layer return volume of images where depth is number of filters you set. 

do more research on stride and receptive field.
how to specify filter in convolutional layer?

activation function used on conv layer as well

pooling - remove dimensionality of input data (used after convolutional layer) (often multiple conv layer then max pooling layer in sequence). 

max pooling find maximum of patches (stride referse to distance movement between each patch)

 stacking convolution + relu + pooling for finding features. Then it is flattened and passed to a few dense layers. 

 cnns used for classification, not necessarily regression

can also be used for semantic segmentation to find what class each pixel in the image is (dynamically labeling of each pixel). just change ooutput dimensions and stuff

Bigger filter size means more of surrounding pixels will be used for feature detection (maybe 11 by 11 or 9 by 9 max)

filter coefficients are learnt during training process

see how to vidualize result of image being passed through convolution filter



